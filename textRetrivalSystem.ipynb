{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCIkn0DxQKlvjttqf+Pbtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LVRANRAN/maching_learning_homework/blob/master/textRetrivalSystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in3oRAdpMLdV",
        "colab_type": "code",
        "outputId": "f397c1ae-1930-4cd6-ffd8-cec3aec692f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "filepath0 = \"d_query.txt\"\n",
        "filepath1 = \"d1.txt\"\n",
        "filepath2 = \"d2.txt\"\n",
        "filepath3 = \"d3.txt\"\n",
        "filepath4 = \"d4.txt\"\n",
        "filepath5 = \"d5.txt\"\n",
        "\n",
        "# parser function\n",
        "def parser(filepath):\n",
        "  file = open(filepath,\"r\")\n",
        "  content = file.read()\n",
        "  file.close()\n",
        "  return content\n",
        "\n",
        "#lowercase letters, remove punctuation and white space\n",
        "document0 = str(parser(filepath0)).lower().translate(str.maketrans(\"\",\"\",string.punctuation)).strip()\n",
        "document1 = str(parser(filepath1)).lower().translate(str.maketrans(\"\",\"\",string.punctuation)).strip()\n",
        "document2 = str(parser(filepath2)).lower().translate(str.maketrans(\"\",\"\",string.punctuation)).strip()\n",
        "document3 = str(parser(filepath3)).lower().translate(str.maketrans(\"\",\"\",string.punctuation)).strip()\n",
        "document4 = str(parser(filepath4)).lower().translate(str.maketrans(\"\",\"\",string.punctuation)).strip()\n",
        "document5 = str(parser(filepath5)).lower().translate(str.maketrans(\"\",\"\",string.punctuation)).strip()\n",
        "\n",
        "#remove the numbers\n",
        "dm0 = re.sub(r'\\d+', '', document0)\n",
        "dm1 = re.sub(r'\\d+', '', document1)\n",
        "dm2 = re.sub(r'\\d+', '', document2)\n",
        "dm3 = re.sub(r'\\d+', '', document3)\n",
        "dm4 = re.sub(r'\\d+', '', document4)\n",
        "dm5 = re.sub(r'\\d+', '', document5)\n",
        "\n",
        "#convert documents to a vector\n",
        "vectorizer1 = CountVectorizer()\n",
        "X = vectorizer1.fit_transform([dm0,dm1,dm2,dm3,dm4,dm5])\n",
        "feature_names1 = vectorizer1.get_feature_names()\n",
        "dense1 = X.todense() #transfer X to a matrix\n",
        "denselist1 = dense1.tolist()\n",
        "vectors = pd.DataFrame(denselist1, columns=feature_names1)\n",
        "print(vectors)\n",
        "\n",
        "#calculate tf-idf\n",
        "vectorizer2 = TfidfVectorizer()\n",
        "Y = vectorizer2.fit_transform([dm0,dm1,dm2,dm3,dm4,dm5])\n",
        "feature_names2 = vectorizer.get_feature_names()\n",
        "dense2 = Y.todense()\n",
        "denselist2 = dense2.tolist()\n",
        "tfidf = pd.DataFrame(denselist2, columns=feature_names2)\n",
        "print(tfidf)\n",
        "\n",
        "#calculater cosime similarity\n",
        "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
        "cosine_similarities"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   about  above  according  accused  acidity  ...  yards  years  yemen  york  zone\n",
            "0      0      0          0        0        0  ...      0      0      0     0     0\n",
            "1      1      0          0        0        0  ...      0      0      0     0     0\n",
            "2      0      1          2        1        0  ...      0      0      0     0     1\n",
            "3      0      0          0        0        0  ...      1      0      0     1     0\n",
            "4      0      0          0        0        1  ...      0      1      1     0     0\n",
            "5      0      0          0        0        0  ...      0      0      0     0     0\n",
            "\n",
            "[6 rows x 538 columns]\n",
            "      about     above  according  ...     yemen      york      zone\n",
            "0  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
            "1  0.054612  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
            "2  0.000000  0.046891   0.093782  ...  0.000000  0.000000  0.046891\n",
            "3  0.000000  0.000000   0.000000  ...  0.000000  0.040155  0.000000\n",
            "4  0.000000  0.000000   0.000000  ...  0.046152  0.000000  0.000000\n",
            "5  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
            "\n",
            "[6 rows x 538 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.06296287, 0.04730374, 0.04629572, 0.27353019,\n",
              "       0.09469602])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    }
  ]
}